{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48cc35ab",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning by Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61084203",
   "metadata": {},
   "source": [
    "- The Iris dataset is a widely used dataset in the field of machine learning and data analysis. It is named after the Iris flower plant and was introduced by the British statistician and biologist Ronald Fisher in 1936. The dataset is frequently used as a beginner's dataset to learn and practice various classification algorithms.\n",
    "\n",
    "- The Iris dataset consists of measurements of four features of three different species of Iris flowers: Setosa, Versicolor, and Virginica\n",
    "\n",
    "- **The four features are:**\n",
    "1) Sepal length (in centimeters)\n",
    "2) Sepal width (in centimeters)\n",
    "3) Petal length (in centimeters)\n",
    "4) Petal width (in centimeters)\n",
    "\n",
    "- In the Iris dataset, the target variable is the species of the Iris flowers. It represents the class or category to which each sample belongs. The target variable is a categorical variable with three possible values: Setosa, Versicolor, and Virginica\n",
    "\n",
    "- For each of the three species, there are 50 samples, resulting in a total of 150 samples in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d872433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, SparkTrials, STATUS_OK, Trials\n",
    "\n",
    "# If you are running Databricks Runtime for Machine Learning, `mlflow` is already installed and you can skip the following line. \n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1140c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the iris dataset from scikit-learn\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64dc1c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0932806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c8367f",
   "metadata": {},
   "source": [
    "### Part 1. Single-machine Hyperopt workflow\n",
    "\n",
    "Here are the steps in a Hyperopt workflow:  \n",
    "1. Define a function to minimize.  \n",
    "2. Define a search space over hyperparameters.  \n",
    "3. Select a search algorithm.  \n",
    "4. Run the tuning algorithm with Hyperopt `fmin()`.\n",
    "\n",
    "For more information, see the [Hyperopt documentation](https://github.com/hyperopt/hyperopt/wiki/FMin)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36692b0",
   "metadata": {},
   "source": [
    "#### 1- Define a function to minimize.\n",
    "We use a support vector machine classifier. The objective is to find the best value for the regularization parameter C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fdabfb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(C):\n",
    "    # Create a support vector classifier model\n",
    "    clf = SVC(C=C)\n",
    "    \n",
    "    # Use the cross-validation accuracy to compare the models' performance\n",
    "    accuracy = cross_val_score(clf, X, y).mean()\n",
    "    \n",
    "    # Hyperopt tries to minimize the objective function. A higher accuracy value means a better model, so you must return the negative accuracy.\n",
    "    return {'loss': -accuracy, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd787a72",
   "metadata": {},
   "source": [
    "#### 2- Define the search space over hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c35e2c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = hp.lognormal('C', 0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b6652c",
   "metadata": {},
   "source": [
    "#### 2- Select a search algorithm\n",
    "The two main choices are:\n",
    "\n",
    "`hyperopt.tpe.suggest`: Tree of Parzen Estimators, a Bayesian approach which iteratively and adaptively selects new hyperparameter settings to explore based on past results.\n",
    "\n",
    "`hyperopt.rand.suggest`: Random search, a non-adaptive approach that samples over the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dc61b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = tpe.suggest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16036e6f",
   "metadata": {},
   "source": [
    "#### 3- Run the tuning algorithm with Gyperopt fmin ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1ffcc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 200/200 [00:04<00:00, 43.55trial/s, best loss: -0.9866666666666667]\n"
     ]
    }
   ],
   "source": [
    "argmin = fmin(\n",
    "  fn=objective,\n",
    "  space=search_space,\n",
    "  algo=algo,\n",
    "  max_evals=200\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20ce740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best value found:  {'C': 4.513431083458916}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best value found: \", argmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a1ad7c",
   "metadata": {},
   "source": [
    "\n",
    "## Part 2. Distributed tuning using Apache Spark and MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d13367",
   "metadata": {},
   "source": [
    "- **Distributed tuning** A technique for tuning the hyperparameters of ML models on large datasets. It works by distributing the tuning process across multiple machines, which can significantly speed up the tuning process\n",
    "\n",
    "- Apache Spark is a distributed computing framework that can be used to run distributed tuning jobs.\n",
    "- MLflow is an open-source platform for managing the end-to-end ML lifecycle, including hyperparameter tuning.\n",
    "\n",
    "To distribute tuning, add one more argument to `fmin()`: Argument `Trials` & class `SparkTrials`\n",
    "\n",
    "`SparkTrials` takes 2 optional arguments:  \n",
    "* `parallelism`: Number of models to fit and evaluate concurrently. The default is the number of available Spark task slots.\n",
    "* `timeout`: Maximum time (in seconds) that `fmin()` can run. The default is no maximum time limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1a3edaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b0a1d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "SparkTrials cannot import pyspark classes.  Make sure that PySpark is available in your environment.  E.g., try running 'import pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m spark_trials \u001b[38;5;241m=\u001b[39m SparkTrials()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\hyperopt\\spark.py:79\u001b[0m, in \u001b[0;36mSparkTrials.__init__\u001b[1;34m(self, parallelism, timeout, loss_threshold, spark_session)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(exp_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, refresh\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _have_spark:\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSparkTrials cannot import pyspark classes.  Make sure that PySpark \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis available in your environment.  E.g., try running \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimport pyspark\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[0;32m     83\u001b[0m validate_timeout(timeout)\n\u001b[0;32m     84\u001b[0m validate_loss_threshold(loss_threshold)\n",
      "\u001b[1;31mException\u001b[0m: SparkTrials cannot import pyspark classes.  Make sure that PySpark is available in your environment.  E.g., try running 'import pyspark'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "from hyperopt import SparkTrials\n",
    "\n",
    "spark_trials = SparkTrials()\n",
    "with mlflow.start_run():\n",
    "  argmin = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=algo,\n",
    "    max_evals=16,\n",
    "    trials=spark_trials\n",
    "  )\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88485cda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
